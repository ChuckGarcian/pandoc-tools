<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc-markdown-css-theme" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Chuck Garcia" />
  <title>Matrix Notes</title>
  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/skylighting-solarized-theme.css" />
  <script defer="" src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css" />
  
</head>
<body>

<header>
<h1 class="title">Matrix Notes</h1>
<blockquote class="metadata">
<p class="subtitle">Notes the fundamentals of the GOTO Algorithm</p>
<p class="author">
      <a href="https://github.com/chuckgarcian">Chuck Garcia</a>
  </p>
<p class="date before-toc"><time datetime="01-30-2024">01-30-2024</time></p>
</blockquote>
</header>

<nav id="TOC" role="doc-toc">
    <a href="..">← Return home</a><br>
    <strong>Contents</strong><label for="contents">⊕</label>
  <input type="checkbox" id="contents">
  <ul>
  <li><a href="#how-changing-mr-and-nr-effect-performance---optimizing-m" id="toc-how-changing-mr-and-nr-effect-performance---optimizing-m">How Changing MR and NR effect performance? - Optimizing <span class="math inline">m</span></a>
  <ul>
  <li><a href="#mr-nr-and-data-movement-amortization---purpose-of-blocking" id="toc-mr-nr-and-data-movement-amortization---purpose-of-blocking">MR, NR and Data Movement Amortization - Purpose of blocking :</a></li>
  <li><a href="#cache-utilization---using-cache-to-reduce-memory-latency" id="toc-cache-utilization---using-cache-to-reduce-memory-latency">Cache Utilization - Using Cache to Reduce Memory Latency:</a></li>
  </ul></li>
  </ul>
</nav>

<main>
<p>In matrix multiplication, there are many ways one might go about implementing an algorithm. At the algorithm abstraction level, any given implementation may not significantly increase or change the computational complexity. However, at the level of hardware, how one orchestrates a matrix multiplication algorithm can significantly alter the runtime, as algorithm implementation strongly corresponds to the amount of hardware utilization.</p>
<p>We can consider the time it takes to carry out matrix multiplication as a function of the total number of operations and the corresponding latency. More specifically, we can imagine the performance time <span class="math inline">T</span> as a function of <span class="math inline">v</span>, the number of computational operations, <span class="math inline">m</span>, the number of memory operations, and the latency to carry out a memory operation <span class="math inline">\delta</span>: <span class="math inline">T(v, m, \delta) = v + m \times \delta</span>. With this (overly) simplified model of net-performance, it becomes clear that we can optimize runtime via any of the three variables:</p>
<blockquote>
<ul>
<li><span class="math inline">v</span>: Optimize the core computations in hardware<br />
</li>
<li><span class="math inline">m</span>: Decrease the number of memory operations<br />
</li>
<li><span class="math inline">\delta</span>: Decrease memory access latency.<br />
We will focus on the last two.</li>
</ul>
</blockquote>
<h2 id="how-changing-mr-and-nr-effect-performance---optimizing-m">How Changing MR and NR effect performance? - Optimizing <span class="math inline">m</span></h2>
<p>Recall that the micro-kernel is:</p>
<pre><code>1. func micro-kernel(B and C)
2. for p in range (NR)
3.  Load Column of A → reg2
4.  Load Row of B  → reg3 
5.  C += A(0,p) × B(p, 0)</code></pre>
<blockquote>
<p><strong>Aside:</strong> Note that that line 5 is the mathamatical expression for what should happen. The actual implemenation expands that into FMA - axpy statements</p>
</blockquote>
While gemm is represented by:<br />

<ol type="1">
<li>func gemm(A,B, C)
<ol type="1">
<li>for i in range (m) // striding by MR every iteration</li>
<li>for j in range (n) // Striding by NR every iteration</li>
<li>Load matrix C → reg1</li>
<li>→ Call micro-kernel</li>
<li>Store Matrix C → memory.</li>
</ol></li>
</ol>
<h3 id="mr-nr-and-data-movement-amortization---purpose-of-blocking">MR, NR and Data Movement Amortization - Purpose of blocking :</h3>
<p>Strictly speaking, in the context of theoretical peak performance, the loads and stores in <strong>MK</strong> represent extraneous memory operations. Ideally, we would only have to load the entirety of matrices <span class="math inline">A</span>, <span class="math inline">B</span>, and <span class="math inline">C</span> once, and then perform the computation entirely within the processor. However, in practice, we are subjected to the constraints of both memory bandwidth and register file size and consequently have to partition the matrix operands.</p>
<p>Consider the following loads and stores:</p>
<blockquote>
<ul>
<li>With every iteration of the innermost loop of <strong>GEMM</strong>, a portion of <span class="math inline">C</span>, say <span class="math inline">C_{ij}</span>, is loaded into the registers.</li>
</ul>
</blockquote>
<blockquote>
<blockquote>
<ul>
<li>We use it as much as we can before the next iteration, and thereafter the partition <span class="math inline">C_{i,j}</span> is not used again.</li>
</ul>
</blockquote>
</blockquote>
<blockquote>
<ul>
<li>In the micro-kernel, column A is used as much as it can be until we can no longer use it.</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>In <strong>MK</strong> line 5, the implementation expands this with consecutive FMA operations, one for each scalar element of B and column of A.</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>In <strong>MK</strong>, we load in a column of A and it stays in the registers until line 5 is complete.</li>
</ul>
</blockquote>
<blockquote>
<p><strong>Note:</strong> Although note that the total number of elements loaded remains constant, it is simply the case that we are bringing more into the registers at one time and performing more operations.</p>
</blockquote>
<p>In all of these cases, increasing the block sizes of MR and NR affects the resulting performance, by way of doing more float operations per load.</p>
<hr />
<p>To understand this more intuitively, we can imagine that by increasing MR and NR, the number of iterations in the main GEMM loop:</p>
<pre><code>1. for i in range(m) // striding by MR every iteration 
2.   for j in range(n) // striding by NR every iteration </code></pre>
<p>decreases in relation to MR and NR. Thus, the total number of loads and stores decreases, but the total number of computations remains the same. This is what we refer to as ‘amortizing the data movement cost’.</p>
<blockquote>
<p><strong>aside:</strong> In a GEMM operation, assuming that matrix data initially exists within memory - in other words, matrix operand data is not produced internally - then there is a minimum number of total memory accesses to carry out the GEMM operation. Such a lower bound is equal to <span class="math inline">mn + nk + mk</span>, as there are <span class="math inline">mn + nk + mk</span> total data elements in memory.</p>
</blockquote>
<p><strong>Summary:</strong> - The larger MR and NR are, the more computations can be performed per memory operation. - The ratio of useful computation to data movement (memory operations like loads and stores) is related to how close to peak performance we are. In other words, the lower the ratio is, the closer to peak performance the system is. - Hence, by increasing MR and NR, we can keep more of matrix C in the register and make the micro-kernel perform more useful computations.</p>
<h3 id="cache-utilization---using-cache-to-reduce-memory-latency">Cache Utilization - Using Cache to Reduce Memory Latency:</h3>
<p>The algorithm <strong>MK</strong> and <strong>GEMM</strong> currently is able to utilize the register file by to reduce the number of memory operations which results in an increase in net performance, optimizing the <span class="math inline">m</span> term in our net-performance function <span class="math inline">T</span>. However we still have yet taken into account the <span class="math inline">\delta</span> term.</p>
<p>In modern systems, memory access latency largely depends on the utilization of the cache hierarchy. Data elements situated in the upper echelons of the hierarchy have much better access latency compared to elements in the lower echelons. As a result, the simple <strong>GEMM</strong> algorithm from above performs much better when the original matrix operands are small enough to be contained entirely within the <span class="math inline">L1</span> cache (topmost cache).</p>
<p>In other words, since these smaller matrix operands can fit in the cache, we can perform <strong>GEMM</strong> with a smaller cost of data movement. However, net performance starts to degrade as the matrix operand size gets too large to fit in the <span class="math inline">L1</span> cache. To combat this degradation, we need ‘more degrees of freedom’ to more effectively utilize the cache hierarchy. Said another way, we will partition the matrix in such a way that leverages the capacity of each cache level as much as possible while still retaining computation correctness.</p>
<h4 id="gemm-v2---a-simple-new-version">GEMM-V2 - A Simple New Version</h4>
<p>Let <span class="math inline">A</span>, <span class="math inline">B</span> and <span class="math inline">C</span> be matrices of size <span class="math inline">m \times k</span>, <span class="math inline">k \times n</span> and <span class="math inline">m \times n</span> respectively. Then the product of A and B can be implemented using GEMM, by partitioning it into submatrices <span class="math inline">m \times k</span>, <span class="math inline">k \times n</span> and <span class="math inline">m \times n</span> that fit into the L1 Cache.</p>
<p><strong>GEMM-V2</strong></p>
<pre><code>1. func Gemm-v2 ():
2.  for i in range(M, m_c):
3.   for j in range (n, n_c):
4.    for p in range ():
5.     Load Cij → L1 Cache 
6.     Load Aip → L1 Cache
7.     Load Bpj → L1 Cache
8.     → Call Gemm</code></pre>
<blockquote>
<p><strong>aside:</strong> Note on line 4 that in practice, there is no instruction to load a data element into cache. This operation is typically handled by the cache controller on behalf of the program. We can generally assume that the load is occurring.</p>
</blockquote>
</main>

<footer>
<p class="signoff">
  <a href="..">← Return home</a>
</p>
</footer>
<script>
;(function() {
  // Non-essential if user has JavaScript off. Just makes checkboxes look nicer.
  var selector = '.task-list > li > input[type="checkbox"]';
  var checkboxes = document.querySelectorAll(selector);
  Array.from(checkboxes).forEach((checkbox) => {
    var wasChecked = checkbox.checked;
    checkbox.disabled = false;
    checkbox.addEventListener('click', (ev) => {ev.target.checked = wasChecked});
  });
})();
</script>
</body>
</html>
